{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf0c648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T20:05:11.429971Z",
     "iopub.status.busy": "2021-12-04T20:05:11.428970Z",
     "iopub.status.idle": "2021-12-04T20:05:17.022463Z",
     "shell.execute_reply": "2021-12-04T20:05:17.021767Z",
     "shell.execute_reply.started": "2021-12-04T19:47:32.188570Z"
    },
    "papermill": {
     "duration": 5.615582,
     "end_time": "2021-12-04T20:05:17.022660",
     "exception": false,
     "start_time": "2021-12-04T20:05:11.407078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import sys\n",
    "sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations import ImageOnlyTransform\n",
    "\n",
    "import timm\n",
    "\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28111daf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T20:05:17.052782Z",
     "iopub.status.busy": "2021-12-04T20:05:17.052201Z",
     "iopub.status.idle": "2021-12-04T20:05:17.055756Z",
     "shell.execute_reply": "2021-12-04T20:05:17.056311Z",
     "shell.execute_reply.started": "2021-12-04T19:47:39.704816Z"
    },
    "papermill": {
     "duration": 0.02217,
     "end_time": "2021-12-04T20:05:17.056442",
     "exception": false,
     "start_time": "2021-12-04T20:05:17.034272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f0897c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T20:05:17.095277Z",
     "iopub.status.busy": "2021-12-04T20:05:17.094643Z",
     "iopub.status.idle": "2021-12-04T20:05:19.792053Z",
     "shell.execute_reply": "2021-12-04T20:05:19.792535Z",
     "shell.execute_reply.started": "2021-12-04T19:47:41.016923Z"
    },
    "papermill": {
     "duration": 2.725292,
     "end_time": "2021-12-04T20:05:19.792696",
     "exception": false,
     "start_time": "2021-12-04T20:05:17.067404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import os\n",
    "\n",
    "train = pd.DataFrame(columns=['file_path','target'])\n",
    "test = pd.DataFrame(columns=['file_path','target'])\n",
    "\n",
    "def get_train_data(path,train):\n",
    "    \n",
    "    for dirname, _, filenames in os.walk(path):\n",
    "        \n",
    "        if dirname.split('/')[-1] == 'meme':\n",
    "            #Memes\n",
    "            for filename in filenames:\n",
    "#                 print(filename)\n",
    "                train = train.append({'file_path':os.path.join(dirname, filename), 'target':1},ignore_index=True)\n",
    "        elif dirname.split('/')[-1] == 'not_meme':\n",
    "            #Not Memes\n",
    "            for filename in filenames:\n",
    "                train = train.append({'file_path':os.path.join(dirname, filename), 'target':0},ignore_index=True)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return train\n",
    "\n",
    "def get_test_data(path,test):\n",
    "    \n",
    "    for dirname, _, filenames in os.walk(path):\n",
    "        if dirname.split('/')[-1] == 'meme':\n",
    "            #Memes\n",
    "            for filename in filenames:\n",
    "                test = test.append({'file_path':os.path.join(dirname, filename), 'target':1},ignore_index=True)\n",
    "        elif dirname.split('/')[-1] == 'not_meme':\n",
    "            #Not Memes\n",
    "            for filename in filenames:\n",
    "                test = test.append({'file_path':os.path.join(dirname, filename), 'target':0},ignore_index=True)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    return test\n",
    "            \n",
    "train = get_train_data('../input/memes-classification-dataset/memes_dataset/train',train)\n",
    "test = get_test_data('../input/memes-classification-dataset/memes_dataset/test',test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d87fcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T20:05:19.823669Z",
     "iopub.status.busy": "2021-12-04T20:05:19.821800Z",
     "iopub.status.idle": "2021-12-04T20:05:19.824349Z",
     "shell.execute_reply": "2021-12-04T20:05:19.824825Z",
     "shell.execute_reply.started": "2021-12-04T19:47:44.054087Z"
    },
    "papermill": {
     "duration": 0.020396,
     "end_time": "2021-12-04T20:05:19.824964",
     "exception": false,
     "start_time": "2021-12-04T20:05:19.804568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    apex=False\n",
    "    debug=False\n",
    "    print_freq=100\n",
    "    num_workers=4\n",
    "    model_name='nfnet_l0'\n",
    "    size=224\n",
    "    scheduler='CosineAnnealingLR' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    epochs=6\n",
    "    #factor=0.2 # ReduceLROnPlateau\n",
    "    #patience=4 # ReduceLROnPlateau\n",
    "    #eps=1e-6 # ReduceLROnPlateau\n",
    "    T_max=6 # CosineAnnealingLR\n",
    "    #T_0=6 # CosineAnnealingWarmRestarts\n",
    "    lr=1e-4\n",
    "    min_lr=1e-6\n",
    "    batch_size=64\n",
    "    weight_decay=1e-6\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=42\n",
    "    target_size=1\n",
    "    target_col='target'\n",
    "    n_fold=4\n",
    "    trn_fold=[0, 1, 2, 3]\n",
    "    train=True\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 1\n",
    "    train = train.sample(n=1000, random_state=CFG.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbeaef29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T20:05:19.857668Z",
     "iopub.status.busy": "2021-12-04T20:05:19.857034Z",
     "iopub.status.idle": "2021-12-04T20:05:19.860737Z",
     "shell.execute_reply": "2021-12-04T20:05:19.860337Z",
     "shell.execute_reply.started": "2021-12-04T19:47:44.108012Z"
    },
    "papermill": {
     "duration": 0.023573,
     "end_time": "2021-12-04T20:05:19.860851",
     "exception": false,
     "start_time": "2021-12-04T20:05:19.837278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train['target'] = label_encoder.fit_transform(train['target'])\n",
    "test['target'] = label_encoder.fit_transform(test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "702a975a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T20:05:19.889290Z",
     "iopub.status.busy": "2021-12-04T20:05:19.888619Z",
     "iopub.status.idle": "2021-12-04T20:05:19.909107Z",
     "shell.execute_reply": "2021-12-04T20:05:19.909531Z",
     "shell.execute_reply.started": "2021-12-04T19:47:44.725357Z"
    },
    "papermill": {
     "duration": 0.037812,
     "end_time": "2021-12-04T20:05:19.909651",
     "exception": false,
     "start_time": "2021-12-04T20:05:19.871839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold  target\n",
       "0     0         125\n",
       "      1         134\n",
       "1     0         125\n",
       "      1         134\n",
       "2     0         126\n",
       "      1         133\n",
       "3     0         125\n",
       "      1         133\n",
       "4     0         125\n",
       "      1         133\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_col])):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby(['fold', 'target']).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "988332d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T20:05:19.936374Z",
     "iopub.status.busy": "2021-12-04T20:05:19.935613Z",
     "iopub.status.idle": "2021-12-04T20:05:19.938017Z",
     "shell.execute_reply": "2021-12-04T20:05:19.937566Z",
     "shell.execute_reply.started": "2021-12-04T19:47:45.516910Z"
    },
    "papermill": {
     "duration": 0.017055,
     "end_time": "2021-12-04T20:05:19.938115",
     "exception": false,
     "start_time": "2021-12-04T20:05:19.921060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_score(y_true, y_pred):\n",
    "    score = roc_auc_score(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ad324f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T20:05:19.968262Z",
     "iopub.status.busy": "2021-12-04T20:05:19.967560Z",
     "iopub.status.idle": "2021-12-04T20:05:19.969905Z",
     "shell.execute_reply": "2021-12-04T20:05:19.969485Z",
     "shell.execute_reply.started": "2021-12-04T19:52:15.568835Z"
    },
    "papermill": {
     "duration": 0.020634,
     "end_time": "2021-12-04T20:05:19.970004",
     "exception": false,
     "start_time": "2021-12-04T20:05:19.949370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['file_path'].values\n",
    "        self.labels = df['target'].values\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_path = self.file_names[index]\n",
    "        image = Image.open(file_path).convert('RGB')\n",
    "#         image = image.astype(np.float32)\n",
    "        image = image.resize((CFG.size,CFG.size))\n",
    "        image = np.array(image)\n",
    "        image = image[:,:,:3]\n",
    "        image = np.moveaxis(image, -1, 0)\n",
    "        image = torch.from_numpy(image).float()\n",
    "        \n",
    "#         if self.transform:\n",
    "#             image = self.transform(image=image)['image']\n",
    "#         else:\n",
    "#             image = image[np.newaxis,:,:]\n",
    "            \n",
    "            \n",
    "        label = torch.tensor(self.labels[index]).float()\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed9cccef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T20:05:19.999240Z",
     "iopub.status.busy": "2021-12-04T20:05:19.998529Z",
     "iopub.status.idle": "2021-12-04T20:05:20.000634Z",
     "shell.execute_reply": "2021-12-04T20:05:20.001024Z",
     "shell.execute_reply.started": "2021-12-04T19:47:46.364219Z"
    },
    "papermill": {
     "duration": 0.019631,
     "end_time": "2021-12-04T20:05:20.001154",
     "exception": false,
     "start_time": "2021-12-04T20:05:19.981523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Transforms\n",
    "# ====================================================\n",
    "def get_transforms(*, data):\n",
    "    \n",
    "    if data == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(CFG.size, CFG.size),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    elif data == 'valid':\n",
    "        return A.Compose([\n",
    "            A.Resize(CFG.size, CFG.size),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f32fc717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T20:05:20.031215Z",
     "iopub.status.busy": "2021-12-04T20:05:20.030473Z",
     "iopub.status.idle": "2021-12-04T20:05:20.032431Z",
     "shell.execute_reply": "2021-12-04T20:05:20.032860Z",
     "shell.execute_reply.started": "2021-12-04T19:47:46.729117Z"
    },
    "papermill": {
     "duration": 0.019808,
     "end_time": "2021-12-04T20:05:20.032978",
     "exception": false,
     "start_time": "2021-12-04T20:05:20.013170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# MODEL\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.model = timm.create_model(self.cfg.model_name, pretrained=pretrained, in_chans=3)\n",
    "        self.n_features = self.model.head.fc.in_features\n",
    "        self.model.head.fc = nn.Linear(self.n_features, self.cfg.target_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab5bc40e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T20:05:20.080680Z",
     "iopub.status.busy": "2021-12-04T20:05:20.064348Z",
     "iopub.status.idle": "2021-12-04T20:05:20.083118Z",
     "shell.execute_reply": "2021-12-04T20:05:20.082739Z",
     "shell.execute_reply.started": "2021-12-04T19:47:47.203038Z"
    },
    "papermill": {
     "duration": 0.037944,
     "end_time": "2021-12-04T20:05:20.083217",
     "exception": false,
     "start_time": "2021-12-04T20:05:20.045273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    if CFG.apex:\n",
    "        scaler = GradScaler()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        if CFG.apex:\n",
    "            with autocast():\n",
    "                y_preds = model(images)\n",
    "                loss = criterion(y_preds.view(-1), labels)\n",
    "        else:\n",
    "            y_preds = model(images)\n",
    "            loss = criterion(y_preds.view(-1), labels)\n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        if CFG.apex:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            if CFG.apex:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  #'LR: {lr:.6f}  '\n",
    "                  .format(\n",
    "                   epoch+1, step, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses,\n",
    "                   remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                   grad_norm=grad_norm,\n",
    "                   #lr=scheduler.get_lr()[0],\n",
    "                   ))\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    scores = AverageMeter()\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (images, labels) in enumerate(valid_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(images)\n",
    "        loss = criterion(y_preds.view(-1), labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        # record accuracy\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(\n",
    "                   step, len(valid_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses,\n",
    "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                   ))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76a6552c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T20:05:20.124797Z",
     "iopub.status.busy": "2021-12-04T20:05:20.113638Z",
     "iopub.status.idle": "2021-12-04T20:05:20.127228Z",
     "shell.execute_reply": "2021-12-04T20:05:20.126782Z",
     "shell.execute_reply.started": "2021-12-04T19:47:47.836058Z"
    },
    "papermill": {
     "duration": 0.033204,
     "end_time": "2021-12-04T20:05:20.127362",
     "exception": false,
     "start_time": "2021-12-04T20:05:20.094158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    trn_idx = folds[folds['fold'] != fold].index\n",
    "    val_idx = folds[folds['fold'] == fold].index\n",
    "\n",
    "    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_col].values\n",
    "\n",
    "    train_dataset = TrainDataset(train_folds, \n",
    "                                 transform=get_transforms(data='train'))\n",
    "    valid_dataset = TrainDataset(valid_folds, \n",
    "                                 transform=get_transforms(data='valid'))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                              batch_size=CFG.batch_size, \n",
    "                              shuffle=True, \n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, \n",
    "                              batch_size=CFG.batch_size * 2, \n",
    "                              shuffle=False, \n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler \n",
    "    # ====================================================\n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        return scheduler\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, pretrained=True)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay, amsgrad=False)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_score = 0.\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(CFG.epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "        \n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(valid_labels, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'preds': preds},\n",
    "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_score.pth')\n",
    "        \n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(), \n",
    "                        'preds': preds},\n",
    "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_loss.pth')\n",
    "    \n",
    "    valid_folds['preds'] = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best_loss.pth', \n",
    "                                      map_location=torch.device('cpu'))['preds']\n",
    "\n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "186572c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T20:05:20.159987Z",
     "iopub.status.busy": "2021-12-04T20:05:20.158933Z",
     "iopub.status.idle": "2021-12-04T20:05:20.160782Z",
     "shell.execute_reply": "2021-12-04T20:05:20.161729Z",
     "shell.execute_reply.started": "2021-12-04T19:47:48.440106Z"
    },
    "papermill": {
     "duration": 0.022602,
     "end_time": "2021-12-04T20:05:20.161883",
     "exception": false,
     "start_time": "2021-12-04T20:05:20.139281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "\n",
    "    \"\"\"\n",
    "    Prepare: 1.train \n",
    "    \"\"\"\n",
    "\n",
    "    def get_result(result_df):\n",
    "        preds = result_df['preds'].values\n",
    "        labels = result_df[CFG.target_col].values\n",
    "        score = get_score(labels, preds)\n",
    "        LOGGER.info(f'Score: {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        # train \n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(train, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        # CV result\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        # save result\n",
    "        oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ec21e34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-04T20:05:20.190188Z",
     "iopub.status.busy": "2021-12-04T20:05:20.189235Z",
     "iopub.status.idle": "2021-12-04T20:18:14.826068Z",
     "shell.execute_reply": "2021-12-04T20:18:14.824335Z"
    },
    "papermill": {
     "duration": 774.652291,
     "end_time": "2021-12-04T20:18:14.826258",
     "exception": false,
     "start_time": "2021-12-04T20:05:20.173967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/nfnet_l0_ra2-45c6688d.pth\" to /root/.cache/torch/hub/checkpoints/nfnet_l0_ra2-45c6688d.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/16] Data 5.792 (5.792) Elapsed 0m 13s (remain 3m 19s) Loss: 0.6158(0.6158) Grad: 20.1955  \n",
      "Epoch: [1][15/16] Data 0.000 (0.572) Elapsed 0m 26s (remain 0m 0s) Loss: 0.3905(0.2565) Grad: 34.8505  \n",
      "EVAL: [0/3] Data 5.755 (5.755) Elapsed 0m 6s (remain 0m 12s) Loss: 0.0965(0.0965) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.2565  avg_val_loss: 0.1133  time: 34s\n",
      "Epoch 1 - Score: 0.9891\n",
      "Epoch 1 - Save Best Score: 0.9891 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (2.035) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0210(0.1133) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Save Best Loss: 0.1133 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/16] Data 5.288 (5.288) Elapsed 0m 5s (remain 1m 29s) Loss: 0.0444(0.0444) Grad: 2.9241  \n",
      "Epoch: [2][15/16] Data 0.000 (0.864) Elapsed 0m 24s (remain 0m 0s) Loss: 0.0639(0.0597) Grad: 19.3333  \n",
      "EVAL: [0/3] Data 5.211 (5.211) Elapsed 0m 5s (remain 0m 11s) Loss: 0.0941(0.0941) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0597  avg_val_loss: 0.0897  time: 32s\n",
      "Epoch 2 - Score: 0.9954\n",
      "Epoch 2 - Save Best Score: 0.9954 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.928) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0029(0.0897) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Save Best Loss: 0.0897 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/16] Data 4.468 (4.468) Elapsed 0m 5s (remain 1m 17s) Loss: 0.0094(0.0094) Grad: 0.7367  \n",
      "Epoch: [3][15/16] Data 0.000 (0.837) Elapsed 0m 24s (remain 0m 0s) Loss: 0.0067(0.0131) Grad: 0.8904  \n",
      "EVAL: [0/3] Data 5.251 (5.251) Elapsed 0m 5s (remain 0m 11s) Loss: 0.0980(0.0980) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0131  avg_val_loss: 0.0934  time: 31s\n",
      "Epoch 3 - Score: 0.9958\n",
      "Epoch 3 - Save Best Score: 0.9958 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.872) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0018(0.0934) \n",
      "Epoch: [4][0/16] Data 5.402 (5.402) Elapsed 0m 6s (remain 1m 31s) Loss: 0.0040(0.0040) Grad: 0.6560  \n",
      "Epoch: [4][15/16] Data 0.000 (0.890) Elapsed 0m 25s (remain 0m 0s) Loss: 0.0007(0.0025) Grad: 0.0795  \n",
      "EVAL: [0/3] Data 5.736 (5.736) Elapsed 0m 6s (remain 0m 12s) Loss: 0.1225(0.1225) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0025  avg_val_loss: 0.1056  time: 32s\n",
      "Epoch 4 - Score: 0.9956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (2.012) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0009(0.1056) \n",
      "Epoch: [5][0/16] Data 5.457 (5.457) Elapsed 0m 6s (remain 1m 32s) Loss: 0.0004(0.0004) Grad: 0.0479  \n",
      "Epoch: [5][15/16] Data 0.000 (0.896) Elapsed 0m 25s (remain 0m 0s) Loss: 0.0014(0.0010) Grad: 0.1107  \n",
      "EVAL: [0/3] Data 5.596 (5.596) Elapsed 0m 5s (remain 0m 11s) Loss: 0.1083(0.1083) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0010  avg_val_loss: 0.1027  time: 32s\n",
      "Epoch 5 - Score: 0.9957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.984) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0008(0.1027) \n",
      "Epoch: [6][0/16] Data 6.952 (6.952) Elapsed 0m 7s (remain 1m 54s) Loss: 0.0004(0.0004) Grad: 0.0480  \n",
      "Epoch: [6][15/16] Data 0.000 (0.934) Elapsed 0m 25s (remain 0m 0s) Loss: 0.0005(0.0008) Grad: 0.0507  \n",
      "EVAL: [0/3] Data 5.431 (5.431) Elapsed 0m 5s (remain 0m 11s) Loss: 0.1131(0.1131) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0008  avg_val_loss: 0.1045  time: 33s\n",
      "Epoch 6 - Score: 0.9956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.937) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0007(0.1045) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 result ==========\n",
      "Score: 0.9954\n",
      "========== fold: 1 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/16] Data 4.735 (4.735) Elapsed 0m 5s (remain 1m 21s) Loss: 1.0735(1.0735) Grad: 47.6859  \n",
      "Epoch: [1][15/16] Data 0.000 (0.899) Elapsed 0m 25s (remain 0m 0s) Loss: 0.1272(0.2905) Grad: 8.1203  \n",
      "EVAL: [0/3] Data 4.503 (4.503) Elapsed 0m 4s (remain 0m 9s) Loss: 0.2033(0.2033) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.2905  avg_val_loss: 0.1960  time: 31s\n",
      "Epoch 1 - Score: 0.9793\n",
      "Epoch 1 - Save Best Score: 0.9793 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.723) Elapsed 0m 5s (remain 0m 0s) Loss: 0.0242(0.1960) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Save Best Loss: 0.1960 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/16] Data 5.290 (5.290) Elapsed 0m 5s (remain 1m 29s) Loss: 0.0859(0.0859) Grad: 3.3118  \n",
      "Epoch: [2][15/16] Data 0.000 (0.922) Elapsed 0m 25s (remain 0m 0s) Loss: 0.0217(0.0567) Grad: 1.7351  \n",
      "EVAL: [0/3] Data 4.657 (4.657) Elapsed 0m 5s (remain 0m 10s) Loss: 0.1797(0.1797) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0567  avg_val_loss: 0.1763  time: 32s\n",
      "Epoch 2 - Score: 0.9837\n",
      "Epoch 2 - Save Best Score: 0.9837 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.734) Elapsed 0m 5s (remain 0m 0s) Loss: 0.0623(0.1763) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Save Best Loss: 0.1763 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/16] Data 5.485 (5.485) Elapsed 0m 6s (remain 1m 32s) Loss: 0.0770(0.0770) Grad: 6.4075  \n",
      "Epoch: [3][15/16] Data 0.000 (0.899) Elapsed 0m 25s (remain 0m 0s) Loss: 0.0046(0.0172) Grad: 0.6101  \n",
      "EVAL: [0/3] Data 4.649 (4.649) Elapsed 0m 5s (remain 0m 10s) Loss: 0.1316(0.1316) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0172  avg_val_loss: 0.1678  time: 31s\n",
      "Epoch 3 - Score: 0.9881\n",
      "Epoch 3 - Save Best Score: 0.9881 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.747) Elapsed 0m 5s (remain 0m 0s) Loss: 0.0180(0.1678) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Save Best Loss: 0.1678 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/16] Data 5.577 (5.577) Elapsed 0m 6s (remain 1m 34s) Loss: 0.0056(0.0056) Grad: 0.6978  \n",
      "Epoch: [4][15/16] Data 0.000 (0.925) Elapsed 0m 25s (remain 0m 0s) Loss: 0.0049(0.0032) Grad: 0.6214  \n",
      "EVAL: [0/3] Data 4.682 (4.682) Elapsed 0m 5s (remain 0m 10s) Loss: 0.1625(0.1625) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0032  avg_val_loss: 0.1826  time: 32s\n",
      "Epoch 4 - Score: 0.9878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.730) Elapsed 0m 5s (remain 0m 0s) Loss: 0.0061(0.1826) \n",
      "Epoch: [5][0/16] Data 5.227 (5.227) Elapsed 0m 5s (remain 1m 29s) Loss: 0.0014(0.0014) Grad: 0.1642  \n",
      "Epoch: [5][15/16] Data 0.000 (0.905) Elapsed 0m 25s (remain 0m 0s) Loss: 0.0007(0.0014) Grad: 0.1054  \n",
      "EVAL: [0/3] Data 4.708 (4.708) Elapsed 0m 5s (remain 0m 10s) Loss: 0.1761(0.1761) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0014  avg_val_loss: 0.1858  time: 31s\n",
      "Epoch 5 - Score: 0.9881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.761) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0048(0.1858) \n",
      "Epoch: [6][0/16] Data 5.272 (5.272) Elapsed 0m 5s (remain 1m 29s) Loss: 0.0017(0.0017) Grad: 0.1407  \n",
      "Epoch: [6][15/16] Data 0.000 (0.920) Elapsed 0m 25s (remain 0m 0s) Loss: 0.0004(0.0010) Grad: 0.0666  \n",
      "EVAL: [0/3] Data 4.718 (4.718) Elapsed 0m 5s (remain 0m 10s) Loss: 0.1765(0.1765) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0010  avg_val_loss: 0.1871  time: 32s\n",
      "Epoch 6 - Score: 0.9881\n",
      "========== fold: 1 result ==========\n",
      "Score: 0.9881\n",
      "========== fold: 2 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.847) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0048(0.1871) \n",
      "Epoch: [1][0/16] Data 4.158 (4.158) Elapsed 0m 4s (remain 1m 12s) Loss: 0.8590(0.8590) Grad: 28.6530  \n",
      "Epoch: [1][15/16] Data 0.000 (0.864) Elapsed 0m 24s (remain 0m 0s) Loss: 0.1065(0.3019) Grad: 11.2391  \n",
      "EVAL: [0/3] Data 5.211 (5.211) Elapsed 0m 5s (remain 0m 11s) Loss: 0.0774(0.0774) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.3019  avg_val_loss: 0.1203  time: 32s\n",
      "Epoch 1 - Score: 0.9933\n",
      "Epoch 1 - Save Best Score: 0.9933 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.944) Elapsed 0m 6s (remain 0m 0s) Loss: 0.9720(0.1203) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Save Best Loss: 0.1203 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/16] Data 5.045 (5.045) Elapsed 0m 5s (remain 1m 26s) Loss: 0.0467(0.0467) Grad: 3.0488  \n",
      "Epoch: [2][15/16] Data 0.000 (0.845) Elapsed 0m 24s (remain 0m 0s) Loss: 0.2469(0.0765) Grad: 12.8911  \n",
      "EVAL: [0/3] Data 5.020 (5.020) Elapsed 0m 5s (remain 0m 10s) Loss: 0.1335(0.1335) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0765  avg_val_loss: 0.0756  time: 31s\n",
      "Epoch 2 - Score: 0.9989\n",
      "Epoch 2 - Save Best Score: 0.9989 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.922) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0271(0.0756) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Save Best Loss: 0.0756 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/16] Data 5.327 (5.327) Elapsed 0m 6s (remain 1m 30s) Loss: 0.0874(0.0874) Grad: 4.1489  \n",
      "Epoch: [3][15/16] Data 0.000 (0.899) Elapsed 0m 25s (remain 0m 0s) Loss: 0.0132(0.0291) Grad: 1.7083  \n",
      "EVAL: [0/3] Data 4.779 (4.779) Elapsed 0m 5s (remain 0m 10s) Loss: 0.0558(0.0558) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0291  avg_val_loss: 0.0362  time: 31s\n",
      "Epoch 3 - Score: 0.9999\n",
      "Epoch 3 - Save Best Score: 0.9999 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.802) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0373(0.0362) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Save Best Loss: 0.0362 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/16] Data 7.528 (7.528) Elapsed 0m 8s (remain 2m 3s) Loss: 0.0038(0.0038) Grad: 0.4722  \n",
      "Epoch: [4][15/16] Data 0.000 (0.925) Elapsed 0m 25s (remain 0m 0s) Loss: 0.0049(0.0099) Grad: 1.1896  \n",
      "EVAL: [0/3] Data 4.674 (4.674) Elapsed 0m 5s (remain 0m 10s) Loss: 0.0164(0.0164) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0099  avg_val_loss: 0.0220  time: 32s\n",
      "Epoch 4 - Score: 1.0000\n",
      "Epoch 4 - Save Best Score: 1.0000 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.771) Elapsed 0m 6s (remain 0m 0s) Loss: 0.1357(0.0220) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Save Best Loss: 0.0220 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/16] Data 5.481 (5.481) Elapsed 0m 6s (remain 1m 32s) Loss: 0.0068(0.0068) Grad: 1.5247  \n",
      "Epoch: [5][15/16] Data 0.165 (0.820) Elapsed 0m 24s (remain 0m 0s) Loss: 0.0020(0.0034) Grad: 0.1776  \n",
      "EVAL: [0/3] Data 4.736 (4.736) Elapsed 0m 5s (remain 0m 10s) Loss: 0.0458(0.0458) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0034  avg_val_loss: 0.0278  time: 30s\n",
      "Epoch 5 - Score: 0.9999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.796) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0411(0.0278) \n",
      "Epoch: [6][0/16] Data 5.510 (5.510) Elapsed 0m 6s (remain 1m 32s) Loss: 0.0015(0.0015) Grad: 0.1421  \n",
      "Epoch: [6][15/16] Data 0.000 (0.859) Elapsed 0m 24s (remain 0m 0s) Loss: 0.0007(0.0016) Grad: 0.1006  \n",
      "EVAL: [0/3] Data 5.407 (5.407) Elapsed 0m 5s (remain 0m 11s) Loss: 0.0526(0.0526) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0016  avg_val_loss: 0.0303  time: 32s\n",
      "Epoch 6 - Score: 0.9999\n",
      "========== fold: 2 result ==========\n",
      "Score: 1.0000\n",
      "========== fold: 3 training ==========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (2.036) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0336(0.0303) \n",
      "Epoch: [1][0/16] Data 5.060 (5.060) Elapsed 0m 5s (remain 1m 26s) Loss: 0.9477(0.9477) Grad: 29.8462  \n",
      "Epoch: [1][15/16] Data 0.000 (0.876) Elapsed 0m 24s (remain 0m 0s) Loss: 0.2141(0.3020) Grad: 8.0208  \n",
      "EVAL: [0/3] Data 4.875 (4.875) Elapsed 0m 5s (remain 0m 10s) Loss: 0.0983(0.0983) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.3020  avg_val_loss: 0.1120  time: 32s\n",
      "Epoch 1 - Score: 0.9919\n",
      "Epoch 1 - Save Best Score: 0.9919 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.963) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0122(0.1120) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Save Best Loss: 0.1120 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/16] Data 6.014 (6.014) Elapsed 0m 6s (remain 1m 40s) Loss: 0.1942(0.1942) Grad: 8.4612  \n",
      "Epoch: [2][15/16] Data 0.000 (0.877) Elapsed 0m 24s (remain 0m 0s) Loss: 0.0494(0.0921) Grad: 3.6446  \n",
      "EVAL: [0/3] Data 5.070 (5.070) Elapsed 0m 5s (remain 0m 10s) Loss: 0.0225(0.0225) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.0921  avg_val_loss: 0.1304  time: 32s\n",
      "Epoch 2 - Score: 0.9971\n",
      "Epoch 2 - Save Best Score: 0.9971 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (2.006) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0289(0.1304) \n",
      "Epoch: [3][0/16] Data 5.163 (5.163) Elapsed 0m 5s (remain 1m 27s) Loss: 0.0420(0.0420) Grad: 2.7265  \n",
      "Epoch: [3][15/16] Data 0.000 (0.903) Elapsed 0m 25s (remain 0m 0s) Loss: 0.0365(0.0369) Grad: 2.8815  \n",
      "EVAL: [0/3] Data 4.927 (4.927) Elapsed 0m 5s (remain 0m 10s) Loss: 0.0504(0.0504) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.0369  avg_val_loss: 0.0777  time: 32s\n",
      "Epoch 3 - Score: 0.9970\n",
      "Epoch 3 - Save Best Loss: 0.0777 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.938) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0153(0.0777) \n",
      "Epoch: [4][0/16] Data 5.251 (5.251) Elapsed 0m 6s (remain 1m 30s) Loss: 0.0091(0.0091) Grad: 0.4857  \n",
      "Epoch: [4][15/16] Data 0.000 (0.887) Elapsed 0m 24s (remain 0m 0s) Loss: 0.0090(0.0132) Grad: 1.1767  \n",
      "EVAL: [0/3] Data 4.714 (4.714) Elapsed 0m 5s (remain 0m 10s) Loss: 0.0557(0.0557) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.0132  avg_val_loss: 0.0809  time: 32s\n",
      "Epoch 4 - Score: 0.9969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.972) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0125(0.0809) \n",
      "Epoch: [5][0/16] Data 4.779 (4.779) Elapsed 0m 5s (remain 1m 21s) Loss: 0.0042(0.0042) Grad: 0.2614  \n",
      "Epoch: [5][15/16] Data 0.000 (0.857) Elapsed 0m 24s (remain 0m 0s) Loss: 0.0038(0.0067) Grad: 0.2810  \n",
      "EVAL: [0/3] Data 4.119 (4.119) Elapsed 0m 4s (remain 0m 8s) Loss: 0.0838(0.0838) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.0067  avg_val_loss: 0.0770  time: 31s\n",
      "Epoch 5 - Score: 0.9960\n",
      "Epoch 5 - Save Best Loss: 0.0770 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.859) Elapsed 0m 6s (remain 0m 0s) Loss: 0.0065(0.0770) \n",
      "Epoch: [6][0/16] Data 4.668 (4.668) Elapsed 0m 5s (remain 1m 19s) Loss: 0.0017(0.0017) Grad: 0.7992  \n",
      "Epoch: [6][15/16] Data 0.000 (0.863) Elapsed 0m 24s (remain 0m 0s) Loss: 0.0021(0.0036) Grad: 0.2868  \n",
      "EVAL: [0/3] Data 3.979 (3.979) Elapsed 0m 4s (remain 0m 8s) Loss: 0.0841(0.0841) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.0036  avg_val_loss: 0.0775  time: 31s\n",
      "Epoch 6 - Score: 0.9961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Data 0.000 (1.657) Elapsed 0m 5s (remain 0m 0s) Loss: 0.0058(0.0775) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 3 result ==========\n",
      "Score: 0.9960\n",
      "========== CV ==========\n",
      "Score: 0.9953\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 794.700593,
   "end_time": "2021-12-04T20:18:18.095191",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-04T20:05:03.394598",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
